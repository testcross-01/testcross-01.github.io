<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>deep-learning | Testcross</title><meta name="keywords" content="deep-learning,machine-learning"><meta name="author" content="testcross"><meta name="copyright" content="testcross"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="深度学习 主要参考资料  《机器学习》西瓜书  《南瓜书》 《动手学深度学习》 — 动手学深度学习 2.0.0-beta1 documentation (d2l.ai) PyTorch深度学习快速入门教程（绝对通俗易懂！）【小土堆】_哔哩哔哩_bilibili 【吃瓜教程】《机器学习公式详解》（南瓜书）与西瓜书公式推导直播合集_哔哩哔哩_bilibili   机器学习学习基础基本术语 关键  机器">
<meta property="og:type" content="article">
<meta property="og:title" content="deep-learning">
<meta property="og:url" content="https://testcross-01.github.io/2022/08/06/deep_learning/index.html">
<meta property="og:site_name" content="Testcross">
<meta property="og:description" content="深度学习 主要参考资料  《机器学习》西瓜书  《南瓜书》 《动手学深度学习》 — 动手学深度学习 2.0.0-beta1 documentation (d2l.ai) PyTorch深度学习快速入门教程（绝对通俗易懂！）【小土堆】_哔哩哔哩_bilibili 【吃瓜教程】《机器学习公式详解》（南瓜书）与西瓜书公式推导直播合集_哔哩哔哩_bilibili   机器学习学习基础基本术语 关键  机器">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://testcross-01.github.io/images/cover/53eTB2uiNRlXwFP.png">
<meta property="article:published_time" content="2022-08-06T07:03:06.000Z">
<meta property="article:modified_time" content="2022-09-29T15:11:02.276Z">
<meta property="article:author" content="testcross">
<meta property="article:tag" content="deep-learning">
<meta property="article:tag" content="machine-learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://testcross-01.github.io/images/cover/53eTB2uiNRlXwFP.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://testcross-01.github.io/2022/08/06/deep_learning/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'deep-learning',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-09-29 23:11:02'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.4.2"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://avatars.githubusercontent.com/u/59674912?v=4" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">7</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">4</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">3</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/images/cover/53eTB2uiNRlXwFP.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Testcross</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">deep-learning</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-08-06T07:03:06.000Z" title="发表于 2022-08-06 15:03:06">2022-08-06</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-09-29T15:11:02.276Z" title="更新于 2022-09-29 23:11:02">2022-09-29</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/machine-learning/">machine-learning</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/machine-learning/deep-learning/">deep-learning</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="deep-learning"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h3 id="深度学习"><a href="#深度学习" class="headerlink" title="深度学习"></a>深度学习</h3><blockquote>
<p>主要参考资料</p>
<ul>
<li>《机器学习》西瓜书 </li>
<li>《南瓜书》</li>
<li><a target="_blank" rel="noopener" href="https://zh.d2l.ai/">《动手学深度学习》 — 动手学深度学习 2.0.0-beta1 documentation (d2l.ai)</a></li>
<li><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1hE411t7RN?spm_id_from=333.851.header_right.fav_list.click&vd_source=addde7e5427c230f6fdc0e4dbac5939d">PyTorch深度学习快速入门教程（绝对通俗易懂！）【小土堆】_哔哩哔哩_bilibili</a></li>
<li><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1Mh411e7VU?spm_id_from=333.788.header_right.fav_list.click&vd_source=addde7e5427c230f6fdc0e4dbac5939d">【吃瓜教程】《机器学习公式详解》（南瓜书）与西瓜书公式推导直播合集_哔哩哔哩_bilibili</a></li>
</ul>
</blockquote>
<h4 id="机器学习学习基础"><a href="#机器学习学习基础" class="headerlink" title="机器学习学习基础"></a>机器学习学习基础</h4><h5 id="基本术语"><a href="#基本术语" class="headerlink" title="基本术语"></a>基本术语</h5><ol>
<li><p>关键</p>
<ul>
<li><p>机器学习三要素（《机器学习》）</p>
<ul>
<li>模型 具体问题 -&gt; 假设空间</li>
<li>策略 评价标准 -&gt; 最优模型的策略（损失函数）</li>
<li>算法 求解损失函数 -&gt; 最优模型</li>
</ul>
</li>
<li><p>关键组件（D2DL）</p>
<ul>
<li>我们可以学习的<em>数据</em>（data）。</li>
<li>如何转换数据的<em>模型</em>（model）。</li>
<li>一个<em>目标函数</em>（objective function），用来量化模型的有效性。</li>
<li>调整模型参数以优化目标函数的<em>算法</em>（algorithm）。</li>
</ul>
</li>
</ul>
</li>
<li><p>数据</p>
<ul>
<li><strong>数据集</strong>：<em>数据集</em>（data set） 数据集由一个个 <em>样本</em>（example，sample）组成</li>
<li><strong>样本</strong>：样本也叫做 <em>数据点</em>（data point）或者 <em>数据实例</em>（data instance）</li>
<li><strong>特征</strong>：每个样本由一组成为 <em>特征</em>（feature，attribute，或 <em>协变量</em>（covariates））的属性组成</li>
<li><strong>特征空间</strong>：属性张成的空间称为 <em>属性空间</em>（attribute space）、<em>样本空间</em>（sample space）或 <em>输入空间</em>，每个样本在这个空间中的坐标向量，称为 <em>特征向量</em>（feature vector）</li>
<li><strong>维数</strong>：当每个样本的特征类别数量都是相同的时候，其特征向量是固定长度的，这个长度被称为数据的<em>维数</em>（dimensionality）</li>
<li><strong>标签</strong>：机器学习会根据属性进行预测，在上面的监督学习问题中，要预测的是一个特殊的属性，它被称为<em>标签</em>（label，或<em>目标</em>（target）） 西瓜中的好瓜或者坏瓜</li>
<li><strong>训练</strong>：从数据中学得到模型的过程称为 <em>学习</em>（learning）或 <em>训练</em>（training）</li>
<li><strong>假设</strong>：学得模型对应关于数据的某种潜在的规律，因此亦称 <em>假设</em>（hypothesis）</li>
</ul>
</li>
<li><p>模型</p>
<p>深度学习与经典方法的区别主要在于：前者关注的功能强大的模型，这些模型由神经网络错综复杂的交织在一起，包含层层数据转换，因此被称为<em>深度学习</em>（deep learning）</p>
</li>
<li><p>目标函数</p>
<ul>
<li><strong>目标函数</strong>： 在机器学习中，我们需要定义模型的优劣程度的度量，这个度量在大多数情况是“可优化”的，我们称之为<em>目标函数</em>（objective function）。 我们通常定义一个目标函数，并希望优化它到最低点。 因为越低越好，所以这些函数有时被称为<em>损失函数</em>（loss function，或cost function）</li>
<li><strong>训练集</strong>和<strong>测试集</strong>： 在一个数据集上，我们通过最小化总损失来学习模型参数的最佳值。 该数据集由一些为训练而收集的样本组成，称为<em>训练数据集</em>（training dataset，或称为<em>训练集</em>（training set））。 然而，在训练数据上表现良好的模型，并不一定在“新数据集”上有同样的效能，这里的“新数据集”通常称为<em>测试数据集</em>（test dataset，或称为<em>测试集</em>（test set））。</li>
</ul>
</li>
<li><p>优化算法</p>
<p>深度学习中，大多流行的优化算法通常基于一种基本方法–<em>梯度下降</em>（gradient descent）</p>
<ul>
<li><strong>梯度下降</strong>：梯度下降法（英语：Gradient descent）是一个一阶<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E6%9C%80%E4%BC%98%E5%8C%96">最优化</a><a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E7%AE%97%E6%B3%95">算法</a>，通常也称为最陡下降法，但是不该与近似积分的最陡下降法（英语：Method of steepest descent）混淆。 要使用梯度下降法找到一个函数的<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E6%9C%80%E5%80%BC">局部极小值</a>，必须向函数上当前点对应<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E6%A2%AF%E5%BA%A6">梯度</a>（或者是近似梯度）的<em>反方向</em>的规定步长距离点进行<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E8%BF%AD%E4%BB%A3">迭代</a>搜索。如果相反地向梯度<em>正方向</em>迭代进行搜索，则会接近函数的<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E6%9C%80%E5%80%BC">局部极大值</a>点；这个过程则被称为梯度上升法。</li>
</ul>
</li>
</ol>
<h5 id="监督学习"><a href="#监督学习" class="headerlink" title="监督学习"></a>监督学习</h5><p> <em>监督学习</em>（supervised learning）擅长在“给定输入特征”的情况下预测标签。 每个“特征-标签”对都称为一个<em>样本</em>（example）。 有时，即使标签是未知的，样本也可以指代输入特征。 我们的目标是生成一个模型，能够将任何输入特征映射到标签，即预测。</p>
 

<ol>
<li><p>回归与分类</p>
<ul>
<li><strong>回归</strong>：当标签是一个任意数值时，我们称之为 <em>回归</em>（regression）问题</li>
<li><strong>分类</strong>：这种“哪一个？”的问题叫做<em>分类</em>（classification）问题。 在<em>分类</em>问题中，我们希望模型能够预测样本属于哪个<em>类别</em>（category，正式称为<em>类</em>（class））</li>
</ul>
</li>
<li><p>其他</p>
<ul>
<li><strong>标记问题</strong>：学习预测不相互排斥的类别的问题称为<em>多标签分类</em>（multi-label classification）</li>
<li><strong>搜索</strong>：有时，我们不仅仅希望输出为一个类别或一个实值。 在信息检索领域，我们希望对一组项目进行排序</li>
<li><strong>推荐系统</strong>：与搜索和排名相关的问题是<em>推荐系统</em>（recommender system），它的目标是向特定用户进行“个性化”推荐</li>
<li><strong>序列学习</strong>：序列学习需要摄取输入序列或预测输出序列，或两者兼而有之。 具体来说，输入和输出都是可变长度的序列</li>
</ul>
</li>
</ol>
<h5 id="无监督学习"><a href="#无监督学习" class="headerlink" title="无监督学习"></a>无监督学习</h5><p> 我们将数据中不含有“目标”的机器学习问题称为<em>无监督学习</em>（unsupervised learning）</p>
<ol>
<li><p>一些主要的议题</p>
<ul>
<li><em>聚类</em>（clustering）问题：没有标签的情况下，我们是否能给数据分类呢？比如，给定一组照片，我们能把它们分成风景照片、狗、婴儿、猫和山峰的照片吗？同样，给定一组用户的网页浏览记录，我们能否将具有相似行为的用户聚类呢？</li>
<li><em>主成分分析</em>（principal component analysis）问题：我们能否找到少量的参数来准确地捕捉数据的线性相关属性？比如，一个球的运动轨迹可以用球的速度、直径和质量来描述。再比如，裁缝们已经开发出了一小部分参数，这些参数相当准确地描述了人体的形状，以适应衣服的需要。另一个例子：在欧几里得空间中是否存在一种（任意结构的）对象的表示，使其符号属性能够很好地匹配?这可以用来描述实体及其关系，例如“罗马” − “意大利” + “法国” = “巴黎”。</li>
<li><em>因果关系</em>（causality）和<em>概率图模型</em>（probabilistic graphical models）问题：我们能否描述观察到的许多数据的根本原因？例如，如果我们有关于房价、污染、犯罪、地理位置、教育和工资的人口统计数据，我们能否简单地根据经验数据发现它们之间的关系？</li>
<li><em>生成对抗性网络</em>（generative adversarial networks）：为我们提供一种合成数据的方法，甚至像图像和音频这样复杂的非结构化数据。潜在的统计机制是检查真实和虚假数据是否相同的测试，它是无监督学习的另一个重要而令人兴奋的领域。</li>
</ul>
</li>
</ol>
<h5 id="模型评估"><a href="#模型评估" class="headerlink" title="模型评估"></a>模型评估</h5><ol>
<li><p>经验误差和过拟合</p>
<ul>
<li><strong>错误率</strong>：通常我们把分类错误的样本数占样本总数的比例称为 <em>错误率</em>（error rate）如果在 $m$ 个样本中有 $a$ 个样本分类错误$ E = a/m $ ；相应的， $1-E$ 称为 <em>精度</em>（accuracy）</li>
<li><strong>误差</strong>：我们把学习器的实际预测输出与样本的真实输出之间的差异称为 <em>误差</em>（error），训练集上的称为 <em>训练误差</em>（training error）或 <em>经验误差</em>（empirical error），在新样本上的误差称为 <em>泛化误差</em>（generalization error）</li>
<li><strong>泛化能力</strong>：对于深度学习或机器学习模型而言，我们不仅要求它对训练数据集有很好的拟合（训练误差），同时也希望它可以对未知数据集（测试集）有很好的拟合结果（泛化能力），所产生的测试误差被称为泛化误差。度量泛化能力的好坏，最直观的表现就是模型的 <em>过拟合</em>（overfitting）和 <em>欠拟合</em>（underfitting）</li>
<li><strong>欠拟合</strong>：欠拟合是指模型不能在训练集上获得足够低的误差。换句换说，就是模型复杂度低，模型在训练集上就表现很差，没法学习到数据背后的规律。</li>
<li><strong>过拟合</strong>：过拟合是指训练误差和测试误差之间的差距太大。换句换说，就是模型复杂度高于实际问题，模型在训练集上表现很好，但在测试集上却表现很差。模型对训练集”死记硬背”（记住了不适用于测试集的训练集性质或特点），没有理解数据背后的规律，泛化能力差。</li>
</ul>
</li>
<li><p>评估方法</p>
<p>通过实验测试来对学习器的泛化误差进行评估并进而做出选择。为此，需使用一个 <em>测试集</em>（testing set）来测试学习器对新样本的判别能力，然后以测试集上的 <em>测试误差</em>（testing error）作为泛化误差的近似。</p>
<ul>
<li><strong>留出法</strong></li>
<li><strong>交叉验证法</strong></li>
<li><strong>自助法</strong></li>
</ul>
</li>
<li><p>性能度量</p>
<p>对于学习器的泛化能力进行评估，不经需要有效可行的实验估计方法，还需要有衡量模型泛化能力的评价标准，这就是 <em>性能度量</em>（performance measure） </p>
<p>例如： 回归问题最常用的性能度量是 <em>均方误差</em>（mean squared error）<br>$$<br>E(f;D)=\frac{1}{m}\sum_{i=1}^n(f(x)-y)^2<br>$$</p>
<ul>
<li><p><strong>错误率与精度</strong>：分类中最常用的两种性能度量</p>
</li>
<li><p><strong>查准率、查全率</strong> : 查准率  $TP / (TP + FP)$ 查全率 $TP / (TP + FN)$  </p>
<table>
<thead>
<tr>
<th align="center">真实情况↓模型预测→</th>
<th align="center">True</th>
<th align="center">False</th>
</tr>
</thead>
<tbody><tr>
<td align="center">True</td>
<td align="center">TP</td>
<td align="center">FN</td>
</tr>
<tr>
<td align="center">False</td>
<td align="center">FP</td>
<td align="center">TN</td>
</tr>
</tbody></table>
</li>
<li><p><strong>ROC 与 AUC</strong>：基于FPR（伪正类率），TPR（真正类率）的曲线，及曲线面积</p>
</li>
<li><p><strong>代价敏感错误率与代价曲线</strong></p>
</li>
</ul>
</li>
<li><p>比较检验</p>
<p>对性能度量的结果进行比较，不是单纯的数值比较。<em>统计假设检验</em>（hypothesis test）位我们进行学习器性能比较提供了重要依据</p>
</li>
</ol>
<h4 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h4><p> <em>回归</em>（regression）是能为一个或多个自变量与因变量之间关系建模的一类方法。 在自然科学和社会科学领域，回归经常用来表示输入和输出之间的关系。</p>
<p> <em>线性回归</em>（linear regression）基于几个简单的假设： 首先，假设自变量 $x$ 和因变量 $y$ 之间的关系是线性的， 即 $y$ 可以表示为x中元素的加权和，这里通常允许包含观测值的一些噪声； 其次，我们假设任何噪声都比较正常，如噪声遵循正态分布</p>
<h5 id="线性回归的基本概念"><a href="#线性回归的基本概念" class="headerlink" title="线性回归的基本概念"></a>线性回归的基本概念</h5><ol>
<li><p>线性模型</p>
<p>在机器学习领域，我们通常使用的是高维数据集，建模时采用线性代数表示法会比较方便。 当我们的输入包含 𝑑 个特征时，我们将预测结果 𝑦̂ （通常使用“尖角”符号表示𝑦的估计值）表示为：</p>
<p>$$<br>𝑦̂ =𝑤1𝑥1+…+𝑤𝑑𝑥𝑑+𝑏.<br>$$</p>
</li>
</ol>
<p>   将所有特征放到向量$\mathbf{x} \in \mathbb{R}^d$中，并将所有权重放到向量$\mathbf{w} \in \mathbb{R}^d$中，我们可以用点积形式来简洁地表达模型：<br>   $$<br>   \hat{y} = \mathbf{w}^\top \mathbf{x} + b.$$<br>   $$</p>
<p>   在开始寻找最好的<em>模型参数</em>（model parameters）$\mathbf{w}$ 和 $b$ 之前，我们还需要两个东西：</p>
<ol>
<li><p><strong>一种模型质量的度量方式</strong>；</p>
</li>
<li><p><strong>一种能够更新模型以提高模型预测质量的方法</strong>。</p>
</li>
<li><p>一些概念</p>
<ul>
<li><strong>损失函数</strong>：在我们开始考虑如何用模型<em>拟合</em>（fit）数据之前，我们需要确定一个拟合程度的度量。<em>损失函数</em>（loss function）能够量化目标的实际值与预测值之间的差距。</li>
<li><strong>解析解</strong>：线性回归的解可以用一个公式简单地表达出来， 这类解叫作<em>解析解</em>（analytical solution）</li>
</ul>
</li>
</ol>
<h5 id="线性回归的实现"><a href="#线性回归的实现" class="headerlink" title="线性回归的实现"></a>线性回归的实现</h5><ol>
<li><p>生成数据集</p>
<p>我们将 <strong>根据带有噪声的线性模型构造一个人造数据集</strong>。<br>$$<br>\mathbf{y}= \mathbf{X} \mathbf{w} + b + \mathbf\epsilon.<br>$$<br>你可以将$\epsilon$视为模型预测和标签时的潜在观测误差。在这里我们认为标准假设成立，即$\epsilon$服从均值为0的正态分布。为了简化问题，我们将标准差设为0.01。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">synthetic_data</span>(<span class="params">w, b, num_examples</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;生成y=Xw+b+噪声&quot;&quot;&quot;</span></span><br><span class="line">    X = torch.normal(<span class="number">0</span>, <span class="number">1</span>, (num_examples, <span class="built_in">len</span>(w)))</span><br><span class="line">    y = torch.matmul(X, w) + b</span><br><span class="line">    y += torch.normal(<span class="number">0</span>, <span class="number">0.01</span>, y.shape) <span class="comment"># 添加噪声</span></span><br><span class="line">    <span class="keyword">return</span> X, y.reshape((-<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li><p>读取数据</p>
<p>定义一个函数， 该函数能打乱数据集中的样本并以小批量方式获取数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">data_iter</span>(<span class="params">batch_size, features, labels</span>):</span><br><span class="line">    num_examples = <span class="built_in">len</span>(features)</span><br><span class="line">    indices = <span class="built_in">list</span>(<span class="built_in">range</span>(num_examples))</span><br><span class="line">    <span class="comment"># 这些样本是随机读取的，没有特定的顺序</span></span><br><span class="line">    random.shuffle(indices)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, num_examples, batch_size):</span><br><span class="line">        batch_indices = torch.tensor(</span><br><span class="line">            indices[i: <span class="built_in">min</span>(i + batch_size, num_examples)])</span><br><span class="line">        <span class="keyword">yield</span> features[batch_indices], labels[batch_indices]</span><br></pre></td></tr></table></figure></li>
<li><p>定义模型、损失函数和优化算法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">w = torch.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=(<span class="number">2</span>,<span class="number">1</span>), requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.zeros(<span class="number">1</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">linreg</span>(<span class="params">X, w, b</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;线性回归模型&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> torch.matmul(X, w) + b</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">squared_loss</span>(<span class="params">y_hat, y</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;均方损失&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> (y_hat - y.reshape(y_hat.shape)) ** <span class="number">2</span> / <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sgd</span>(<span class="params">params, lr, batch_size</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;小批量随机梯度下降&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">            param -= lr * param.grad / batch_size</span><br><span class="line">            param.grad.zero_()</span><br></pre></td></tr></table></figure></li>
<li><p>训练</p>
<p>我们将执行以下循环：</p>
<ul>
<li>初始化参数</li>
<li>重复以下训练，直到完成<ul>
<li>计算梯度  $\mathbf{g} \leftarrow \partial_{(\mathbf{w},b)} \frac{1}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} l(\mathbf{x}^{(i)}, y^{(i)}, \mathbf{w}, b)$</li>
<li>更新参数  $(\mathbf{w}, b) \leftarrow (\mathbf{w}, b) - \eta \mathbf{g}$</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">lr = <span class="number">0.03</span></span><br><span class="line">num_epochs = <span class="number">3</span></span><br><span class="line">net = linreg</span><br><span class="line">loss = squared_los</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter(batch_size, features, labels):</span><br><span class="line">        l = loss(net(X, w, b), y)  <span class="comment"># X和y的小批量损失</span></span><br><span class="line">        <span class="comment"># 因为l形状是(batch_size,1)，而不是一个标量。l中的所有元素被加到一起，</span></span><br><span class="line">        <span class="comment"># 并以此计算关于[w,b]的梯度</span></span><br><span class="line">        l.<span class="built_in">sum</span>().backward()</span><br><span class="line">        sgd([w, b], lr, batch_size)  <span class="comment"># 使用参数的梯度更新参数</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        train_l = loss(net(features, w, b), labels)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;epoch <span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>, loss <span class="subst">&#123;<span class="built_in">float</span>(train_l.mean()):f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure></li>
</ol>
<h5 id="softmax回归（分类问题）"><a href="#softmax回归（分类问题）" class="headerlink" title="softmax回归（分类问题）"></a>softmax回归（分类问题）</h5><ol>
<li><p>softmax计算</p>
<p>社会科学家邓肯·卢斯于1959年在<em>选择模型</em>（choice model）的理论基础上发明的<em>softmax函数</em>正是这样做的：<br>softmax函数能够将未规范化的预测变换为非负数并且总和为1，同时让模型保持可导的性质。<br>为了完成这一目标，我们首先对每个未规范化的预测求幂，这样可以确保输出非负。<br>为了确保最终输出的概率值总和为1，我们再让每个求幂后的结果除以它们的总和。如下式：<br>$$<br>\hat{\mathbf{y}} = \mathrm{softmax}(\mathbf{o})\quad \text{其中}\quad \hat{y}_j = \frac{\exp(o_j)}{\sum_k \exp(o_k)}<br>$$<br>矢量计算表达式：<br>$$<br>\begin{aligned} \mathbf{O} &amp;= \mathbf{X} \mathbf{W} + \mathbf{b}, \ \hat{\mathbf{Y}} &amp; = \mathrm{softmax}(\mathbf{O}). \end{aligned}<br>$$</p>
</li>
<li><p>网络架构</p>
<p>与线性回归一样，softmax回归也是一个单层神经网络。由于计算每个输出$o_1$、$o_2$和$o_3$取决于所有输入$x_1$、$x_2$、$x_3$和$x_4$，所以softmax回归的输出层也是全连接层。</p>


<p>为了更简洁地表达模型，我们仍然使用线性代数符号。通过向量形式表达为$\mathbf{o} = \mathbf{W} \mathbf{x} + \mathbf{b}$，这是一种更适合数学和编写代码的形式。<br>由此，我们已经将所有权重放到一个$3 \times 4$矩阵中。对于给定数据样本的特征$\mathbf{x}$，我们的输出是由权重与输入特征进行矩阵-向量乘法再加上偏置$\mathbf{b}$得到的。</p>
</li>
<li><p>损失函数</p>
<p>假设整个数据集${\mathbf{X}, \mathbf{Y}}$具有$n$个样本，其中索引$i$的样本由特征向量$\mathbf{x}^{(i)}$和独热标签向量$\mathbf{y}^{(i)}$组成。<br>我们可以将估计值与实际值进行比较：<br>$$<br>P(\mathbf{Y} \mid \mathbf{X}) = \prod_{i=1}^n P(\mathbf{y}^{(i)} \mid \mathbf{x}^{(i)}).<br>$$<br>根据最大似然估计，我们最大化$P(\mathbf{Y} \mid \mathbf{X})$，相当于最小化负对数似然：</p>
<p>$$<br>-\log P(\mathbf{Y} \mid \mathbf{X}) = \sum_{i=1}^n -\log P(\mathbf{y}^{(i)} \mid \mathbf{x}^{(i)})<br>= \sum_{i=1}^n l(\mathbf{y}^{(i)}, \hat{\mathbf{y}}^{(i)}),<br>$$<br>其中，对于任何标签$\mathbf{y}$和模型预测$\hat{\mathbf{y}}$，损失函数为（<em>交叉熵损失</em>（cross-entropy loss））：<br>$$<br>l(\mathbf{y}, \hat{\mathbf{y}}) = - \sum_{j=1}^q y_j \log \hat{y}<em>j.<br>$$<br><strong>注解</strong>：$ -\log P(\mathbf{y}^{(i)} \mid \mathbf{x}^{(i)})$ 到 $- \sum</em>{j=1}^q y_j \log \hat{y}_j.$ 的替换，用概念上理解就以特征矩阵 $\mathbf{x}^{(i)}$为前提，得到 $\mathbf{y}^{(i)}$ 的概率，而式二则是解释如何具体的计算 $q$ 个事件分别发生的概率 乘以 对应条件概率 如： $\hat{y}_1$=$P(y=\text{猫} \mid \mathbf{x})$</p>
</li>
<li><p>信息论基础</p>
<ul>
<li><p><strong>熵</strong>：信息论的核心思想是量化数据中的信息内容。<br>在信息论中，该数值被称为分布$P$的<em>熵</em>（entropy）。可以通过以下方程得到：<br>$$<br>H[P] = \sum_j - P(j) \log P(j).<br>$$</p>
</li>
<li><p><strong>信息量</strong>：克劳德·香农决定用信息量$\log \frac{1}{P(j)} = -\log P(j)$来量化这种惊异程度。在观察一个事件$j$时，并赋予它（主观）概率$P(j)$。当我们赋予一个事件较低的概率时，我们的惊异会更大，该事件的信息量也就更大。</p>
</li>
<li><p><strong>交叉熵</strong>：主要用于度量两个概率分布间的差异性信息。给定两个概率分布p和q，p相对于q的交叉熵定义为<br>$$<br>{\displaystyle H(p,q)=-\sum _{x}p(x),\log q(x).!}<br>$$</p>
</li>
</ul>
</li>
<li><p>softmax及其导数</p>
<p>利用softmax的定义，我们得到：</p>
<p>$$<br>\begin{aligned}<br>l(\mathbf{y}, \hat{\mathbf{y}}) &amp;=  - \sum_{j=1}^q y_j \log \frac{\exp(o_j)}{\sum_{k=1}^q \exp(o_k)} \<br>&amp;= \sum_{j=1}^q y_j \log \sum_{k=1}^q \exp(o_k) - \sum_{j=1}^q y_j o_j\<br>&amp;= \log \sum_{k=1}^q \exp(o_k) - \sum_{j=1}^q y_j o_j.<br>\end{aligned}<br>$$</p>
<p>考虑相对于任何未规范化的预测$o_j$的导数，我们得到：</p>
<p>$$<br>\partial_{o_j} l(\mathbf{y}, \hat{\mathbf{y}}) = \frac{\exp(o_j)}{\sum_{k=1}^q \exp(o_k)} - y_j = \mathrm{softmax}(\mathbf{o})_j - y_j.<br>$$<br>换句话说，导数是我们softmax模型分配的概率与实际发生的情况（由独热标签向量表示）之间的差异。</p>
</li>
</ol>
<h4 id="多层感知机"><a href="#多层感知机" class="headerlink" title="多层感知机"></a>多层感知机</h4><h5 id="多层感知机基本概念"><a href="#多层感知机基本概念" class="headerlink" title="多层感知机基本概念"></a>多层感知机基本概念</h5><ol>
<li><p>线性模型的局限性</p>
<p>线性意味着单调假设： 任何特征的增大都会导致模型输出的增大（如果对应的权重为正）， 或者导致模型输出的减小（如果对应的权重为负）。有时是有道理的，然而我们可以很容易找出违反单调性的例子。 例如，我们想要根据体温预测死亡率。 对体温高于37摄氏度的人来说，温度越高风险越大。 然而，对体温低于37摄氏度的人来说，温度越高风险就越低。</p>
</li>
<li><p>隐藏层</p>
<ul>
<li><p><strong>隐藏层</strong>：输出层与输入层之间层，被称为隐藏层。</p>
</li>
<li><p><strong>克服线性模型的限制</strong>：通过向网络中添加一个或者多个隐藏层来客服线性模型的限制。使其能处理更普遍的函数关系类型。</p>
</li>
<li><p><strong>从线性到非线性</strong>：因为仿射函数的仿射函数任然是仿射函数，而我们之前的线性模型已经可以表示任何仿射函数了。<br>$$<br>\mathbf{O} = (\mathbf{X} \mathbf{W}^{(1)} + \mathbf{b}^{(1)})\mathbf{W}^{(2)} + \mathbf{b}^{(2)} = \mathbf{X} \mathbf{W}^{(1)}\mathbf{W}^{(2)} + \mathbf{b}^{(1)} \mathbf{W}^{(2)} + \mathbf{b}^{(2)} = \mathbf{X} \mathbf{W} + \mathbf{b}.<br>$$<br>因此为了发挥多层架构的潜力，我们还需要一个额外的关键要素：在仿射变换之后对每个隐藏单元应用非线性的<em><strong>激活函数</strong></em>（activation function）$\sigma$。<br>激活函数的输出（例如，$\sigma(\cdot)$）被称为<em>活性值</em>（activations）。<br>一般来说，有了激活函数，就不可能再将我们的多层感知机退化成线性模型：<br>$$<br>\begin{aligned}</p>
<pre><code>\mathbf&#123;H&#125; &amp; = \sigma(\mathbf&#123;X&#125; \mathbf&#123;W&#125;^&#123;(1)&#125; + \mathbf&#123;b&#125;^&#123;(1)&#125;), \\
\mathbf&#123;O&#125; &amp; = \mathbf&#123;H&#125;\mathbf&#123;W&#125;^&#123;(2)&#125; + \mathbf&#123;b&#125;^&#123;(2)&#125;.\\
</code></pre>
<p>\end{aligned}<br>$$</p>
</li>
<li><p><strong>通用近似定理</strong>：此定理意味着神经网络可以用来近似任意的复杂函数，并且可以达到任意近似精准度。但它并没有说明要如何选择神经网络参数（权重、神经元数量、神经层层数等等）来达到想近似的目标函数。</p>
</li>
</ul>
</li>
<li><p>激活函数</p>
<p><em>激活函数</em>（activation function）通过计算加权和并加上偏置来确定神经元是否应该被激活， 它们将输入信号转换为输出的可微运算。</p>
<ul>
<li><p>ReLU函数</p>
<p>最受欢迎的激活函数是<em>修正线性单元</em>（Rectified linear unit，<em>ReLU</em>），因为它实现简单，同时在各种预测任务中表现良好。<br>$$<br>\operatorname{ReLU}(x) = \max(x, 0).<br>$$</p>
</li>
<li><p>sigmoid函数</p>
<p><em>sigmoid函数</em>将输入变换为区间(0, 1)上的输出。因此，sigmoid通常称为<em>挤压函数</em>（squashing function）：<br>它将范围（-inf, inf）中的任意输入压缩到区间（0, 1）中的某个值：<br>$$<br>\operatorname{sigmoid}(x) = \frac{1}{1 + \exp(-x)}.<br>$$</p>
</li>
<li><p>tanh函数</p>
<p>与sigmoid函数类似，tanh(双曲正切)函数也能将其输入压缩转换到区间(-1, 1)上。<br>tanh函数的公式如下：<br>$$<br>\operatorname{tanh}(x) = \frac{1 - \exp(-2x)}{1 + \exp(-2x)}.<br>$$</p>
</li>
</ul>
</li>
</ol>
<h5 id="模型选择、欠拟合和过拟合"><a href="#模型选择、欠拟合和过拟合" class="headerlink" title="模型选择、欠拟合和过拟合"></a>模型选择、欠拟合和过拟合</h5><ol>
<li><p>训练误差和泛化误差</p>
<ul>
<li><strong>训练误差</strong>：<em>训练误差</em>（training error）是指，模型在训练数据集上计算得到的误差。</li>
<li><strong>泛化误差</strong>：<em>泛化误差</em>（generalization error）是指，模型应用在同样从原始样本的分布中抽取的无限多数据样本时，模型误差的期望。</li>
</ul>
<p>在现实中我们无法准确地计算出泛化误差，因为无限多的样本是一个虚构的对象，在现实中只能通过将模型应用于一个独立的测试集来估计泛化误差。</p>
</li>
<li><p>模型复杂性</p>
<p>模型复杂性的构成很复杂，通常对于神经网络，我们认为需要更多训练迭代的模型比较复杂，而需要“早停”（early stopping）的模型（即较少训练迭代周期）就不那么复杂。</p>
<p>一些影响模型泛化的因素：</p>
<ul>
<li>可调整参数的数量。当可调整参数的数量（有时称为<em>自由度</em>）很大时，模型往往更容易过拟合。</li>
<li>参数采用的值。当权重的取值范围较大时，模型可能更容易过拟合。</li>
<li>训练样本的数量。即使你的模型很简单，也很容易过拟合只包含一两个样本的数据集。而过拟合一个有数百万个样本的数据集则需要一个极其灵活的模型。</li>
</ul>
</li>
<li><p>模型选择</p>
<p>在机器学习中，我们通常在评估几个候选模型后选择最终的模型。这个过程叫做<em>模型选择</em>。</p>
<ul>
<li><strong>验证集</strong>：在确定所有的超参数前，我们不应该使用测试集。在模型选择过程中使用了测试集，会有过拟合的风险（如果过拟合了测试集，测试集本身就没意义了）。并且我们也不能只依靠训练集来进行评估，因此我们额外增加了一个<em>验证数据集</em>（validation dataset）， 也叫<em>验证集</em>（validation set）。</li>
</ul>
</li>
<li><p>欠拟合和过拟合</p>
<ul>
<li><p><strong>欠拟合</strong>：训练误差和验证误差都很严重，但两者差距不大。 这种现象被称为<em>欠拟合</em>（underfitting）。</p>
</li>
<li><p><strong>过拟合</strong>：训练误差明显低于验证误差时，这表明严重的<em>过拟合</em>（overfitting）。</p>
</li>
<li><p><strong>影响拟合的重要因素</strong>：模型复杂度、数据集大小</p>
<p>多项式中，高阶意味这更为函数更为复杂。高阶多项式的参数较多，模型函数的选择范围较广。 因此在固定训练数据集的情况下， 高阶多项式函数相对于低阶多项式的训练误差应该始终更低（最坏也是相等）。</p>


<p>训练数据集中的样本越少，我们就越有可能（且更严重地）过拟合。 随着训练数据量的增加，泛化误差通常会减小。 此外，一般来说，更多的数据不会有什么坏处。</p>
</li>
</ul>
</li>
</ol>
<h5 id="权重衰减"><a href="#权重衰减" class="headerlink" title="权重衰减"></a>权重衰减</h5><ol>
<li><p>范数与权重衰减</p>
<ul>
<li><p><strong>范数</strong>：线性代数中最有用的一些运算符是<em>范数</em>（norm）。 非正式地说，一个向量的<em>范数</em>告诉我们一个向量有多大。 这里考虑的<em>大小</em>（size）概念不涉及维度，而是分量的大小。<br>$$<br>|\mathbf{x}|<em>p = \left(\sum</em>{i=1}^n \left|x_i \right|^p \right)^{1/p}.<br>$$</p>
</li>
<li><p><strong>权重衰减</strong>：<em>权重衰减</em>（weight decay）是最广泛使用的正则化的技术之一， 它通常也被称为$L_2$<em>正则化</em>。</p>
<p>这项技术通过函数与零的距离来衡量函数的复杂度，因为在所有函数$f$中，函数$f = 0$（所有输入都得到值$0$）在某种意义上是最简单的。</p>
<p>最常用方法是将其范数作为惩罚项加到最小化损失的问题中， 将原来的训练目标<em>最小化训练标签上的预测损失</em>， 调整为<em>最小化预测损失和惩罚项之和</em>。</p>
</li>
</ul>
</li>
</ol>
<h5 id="暂退法"><a href="#暂退法" class="headerlink" title="暂退法"></a>暂退法</h5><ol>
<li><p>重新审视过拟合</p>
<p>泛化性和灵活性之间的这种基本权衡被描述为<em>偏差-方差权衡</em>（bias-variance tradeoff）。</p>
<p>与线性模型不同，神经网络并不局限于单独查看每个特征，而是学习特征之间的交互。即使我们有比特征值多的多的样本，深度神经网络也有可能过拟合。深度网络的泛化性质令人费解，而这种泛化性质的数学基础仍然是悬而未决的研究问题。</p>
</li>
<li><p>“好”的模型</p>
<p>经典泛化理论认为，为了缩小训练和测试性能之间的差距，应该以简单的模型为目标。</p>
<ul>
<li>简单性以较小维度下形式展现。</li>
<li>简单性表现出平滑性，即函数不应该对其输入的微小变化敏感。Bishop.1995</li>
</ul>
</li>
<li><p>暂退法</p>
<p>如何将毕晓普的想法应用于网络的内部层提出了一个想法：</p>
<p>在训练过程中，他们建议在计算后续层之前向网络的每一层注入噪声。因为当训练一个有多层的深层网络时，注入噪声只会在输入-输出映射上增强平滑性。</p>
<p>这种方法之所以被称为<em>暂退法</em>，因为我们从表面上看是在训练过程中<strong>丢弃（drop out）一些神经元</strong>。</p>
<ul>
<li><p><strong>如何注入噪声</strong>：一种想法是以一种<em>无偏向</em>（unbiased）的方式注入噪声。<br>这样在固定住其他层时，每一层的期望值等于没有噪音时的值。</p>
</li>
<li><p><strong>毕晓普的思路</strong>：在毕晓普的工作中，他将高斯噪声添加到线性模型的输入中。在每次训练迭代中，他将从均值为零的分布$\epsilon \sim \mathcal{N}(0,\sigma^2)$采样噪声添加到输入$\mathbf{x}$，从而产生扰动点$\mathbf{x}’ = \mathbf{x} + \epsilon$，预期是$E[\mathbf{x}’] = \mathbf{x}$。</p>
</li>
<li><p><strong>标准暂退法</strong>：在标准暂退法正则化中，通过按保留（未丢弃）的节点的分数进行规范化来消除每一层的偏差。<br>换言之，每个中间活性值$h$以<em>暂退概率</em>$p$由随机变量$h’$替换，如下所示：<br>$$<br>\begin{aligned}<br>h’ =<br>\begin{cases}</p>
<pre><code>0 &amp; \text&#123; 概率为 &#125; p \\
\frac&#123;h&#125;&#123;1-p&#125; &amp; \text&#123; 其他情况&#125;
</code></pre>
<p>\end{cases}<br>\end{aligned}<br>$$</p>
<p>根据此模型的设计，其期望值保持不变，即$E[h’] = h$。</p>
</li>
</ul>
</li>
</ol>
<h5 id="前向传播、反向传播"><a href="#前向传播、反向传播" class="headerlink" title="前向传播、反向传播"></a>前向传播、反向传播</h5><ol>
<li><p>前向传播</p>
<p><em>前向传播</em>（forward propagation或forward pass） 指的是：按顺序（从输入层到输出层）计算和存储神经网络中每层的结果。</p>
</li>
<li><p>反向传播</p>
<p><em>反向传播</em>（backward propagation或backpropagation）指的是计算神经网络参数梯度的方法。 简言之，该方法根据微积分中的<em>链式规则</em>，按相反的顺序从输出层到输入层遍历网络。该算法存储了计算某些参数梯度时所需的任何中间变量（偏导数）。</p>
</li>
<li><p>训练神经网络</p>
<p>在训练神经网络时，前向传播和反向传播相互依赖。 对于前向传播，我们沿着依赖的方向遍历计算图并计算其路径上的所有变量。 然后将这些用于反向传播，其中计算顺序与计算图的相反。前向传播再利用反向传播给出的梯度来更新模型参数，如此往复。</p>
</li>
</ol>


<blockquote>
<p>具体公式详见 <a target="_blank" rel="noopener" href="http://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html">4.7. 前向传播、反向传播和计算图 — 动手学深度学习 2.0.0-beta1 documentation (d2l.ai)</a></p>
</blockquote>
<h5 id="数值稳定性和模型初始化"><a href="#数值稳定性和模型初始化" class="headerlink" title="数值稳定性和模型初始化"></a>数值稳定性和模型初始化</h5><p>初始化方案的选择再神经网络学习中起举足轻重的作用，它对保持数值稳定性至关重要。</p>
<ol>
<li><p>梯度消失和梯度爆炸</p>
<p>考虑一个具有$L$层、输入$\mathbf{x}$和输出$\mathbf{o}$的深层网络。每一层$l$由变换$f_l$定义，该变换的参数为权重$\mathbf{W}^{(l)}$，其隐藏变量是$\mathbf{h}^{(l)}$（令 $\mathbf{h}^{(0)} = \mathbf{x}$）。我们的网络可以表示为：<br>$$<br>mathbf{h}^{(l)} = f_l (\mathbf{h}^{(l-1)}) \text{ 因此 } \mathbf{o} = f_L \circ \ldots \circ f_1(\mathbf{x}).<br>$$<br>如果所有隐藏变量和输入都是向量，我们可以将$\mathbf{o}$关于任何一组参数$\mathbf{W}^{(l)}$的梯度写为下式：<br>$$<br>\partial_{\mathbf{W}^{(l)}} \mathbf{o} = \underbrace{\partial_{\mathbf{h}^{(L-1)}} \mathbf{h}^{(L)}}<em>{ \mathbf{M}^{(L)} \stackrel{\mathrm{def}}{=}} \cdot \ldots \cdot \underbrace{\partial</em>{\mathbf{h}^{(l)}} \mathbf{h}^{(l+1)}}<em>{ \mathbf{M}^{(l+1)} \stackrel{\mathrm{def}}{=}} \underbrace{\partial</em>{\mathbf{W}^{(l)}} \mathbf{h}^{(l)}}_{ \mathbf{v}^{(l)} \stackrel{\mathrm{def}}{=}}.<br>$$</p>
</li>
</ol>
<p>   换言之，该梯度是$L-l$个矩阵$\mathbf{M}^{(L)} \cdot \ldots \cdot \mathbf{M}^{(l+1)}$与梯度向量 $\mathbf{v}^{(l)}$的乘积。<strong>连续的乘积可能导致结果很大，或者非常小</strong>。</p>
<ul>
<li><p><strong>梯度爆炸</strong>：<em>梯度爆炸</em>（gradient exploding）， 参数更新过大，破坏了模型的稳定收敛。</p>
</li>
<li><p><strong>梯度消失</strong>：<em>梯度消失</em>（gradient vanishing）， 参数更新过小，在每次更新时几乎不会移动，导致模型无法学习。</p>
</li>
<li><p><strong>对称性</strong>：</p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/zzflybird/p/16036923.html">什么是神经网络的对称性/对称状态？ - zzflybird - 博客园 (cnblogs.com)</a></p>
<p>如果<strong>权值初始化</strong>是相等的（这句话的意思是<strong>hidden layer的每一列上的神经元参数都一致</strong>），那么在Back-Propagation的时候，<strong>每个神经元获得的update都是一致的</strong>，那么<strong>更新后的神经元的权重还是一致的</strong>，就导致了网络进入了<strong>对称状态</strong>，所谓<strong>对称</strong>就是<strong>相对于某层hidden layer来讲，其中的所有neural都是一模一样的</strong>，这样我们的网络就不能学到更多的特征了（假想我们的CNN的深层有512个hidden unit，如果这512个单元的值都是一样的，不是白学了么。）</p>
</blockquote>
</li>
</ul>
<ol start="2">
<li><p>参数初始化</p>
<p>解决（或至少减轻）上述问题的一种方法是进行参数初始化， 优化期间的注意和适当的正则化也可以进一步提高稳定性。</p>
<ul>
<li><p><strong>默认初始化</strong>：利用正态分布来初始化权重值。</p>
</li>
<li><p><strong>Xavier初始化</strong>：同时限制输入和输出的方差，满足:<br>$$<br>\begin{aligned}<br>\frac{1}{2} (n_\mathrm{in} + n_\mathrm{out}) \sigma^2 = 1 \text{ 或等价于 }<br>\sigma = \sqrt{\frac{2}{n_\mathrm{in} + n_\mathrm{out}}}.<br>\end{aligned}<br>$$<br>这就是现在标准且实用的<em>Xavier初始化</em>的基础，通常，Xavier初始化从均值为零，方差$\sigma^2 = \frac{2}{n_\mathrm{in} + n_\mathrm{out}}$的高斯分布中采样权重。</p>
</li>
</ul>
</li>
</ol>
<h5 id="环境和分布偏移（暂时未弄懂）"><a href="#环境和分布偏移（暂时未弄懂）" class="headerlink" title="环境和分布偏移（暂时未弄懂）"></a>环境和分布偏移（暂时未弄懂）</h5><ol>
<li>分布偏移的类型<ul>
<li><strong>协变量偏移</strong>：我们假设，虽然输入的分布可能随时间而改变，但标签函数（即条件分布$P(y \mid \mathbf{x})$）没有改变。统计学家称之为<em>协变量偏移</em>（covariate shift）， 因为这个问题是由于协变量（特征）分布的变化而产生的。 </li>
<li><strong>标签偏移</strong>：<em>标签偏移</em>（label shift）描述了与协变量偏移相反的问题。这里我们假设标签边缘概率$P(y)$可以改变，但是类别条件分布$P(\mathbf{x} \mid y)$​在不同的领域之间保持不变。例如，预测患者的疾病，我们可能根据症状来判断，即使疾病的相对流行率随着时间的推移而变化。标签偏移在这里是恰当的假设，因为疾病会引起症状。</li>
<li><strong>概念偏移</strong>：当标签的定义发生变化时，就会出现这种问题。如果我们要建立一个机器翻译系统，$P(y \mid \mathbf{x})$的分布可能会因我们的位置不同而得到不同的翻译。</li>
</ul>
</li>
<li>分布偏移纠正（暂时未弄懂）</li>
</ol>
<h4 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h4><h5 id="从全链接层到卷积"><a href="#从全链接层到卷积" class="headerlink" title="从全链接层到卷积"></a>从全链接层到卷积</h5><ol>
<li><p>不变性</p>
<ul>
<li><strong>平移不变性（translation invariance）</strong>：不管检测对象出现在图像的哪个位置，神经网络的前面几层应该对相同的图像区域具有相似的反应。</li>
<li><strong>局部性（locality）</strong>：神经网络的前面几层应该只探究输入图像中的局部区域，而不过度在意图像中相隔较远区域的关系。</li>
</ul>
</li>
<li><p>对多层感知机的约束</p>
<p>使用$[\mathbf{X}]<em>{i, j}$和$[\mathbf{H}]</em>{i, j}$分别表示输入图像和隐藏表示中位置（$i$,$j$）处的像素。为了使每个隐藏神经元都能接收到每个输入像素的信息，我们将参数从权重矩阵（如同我们先前在多层感知机中所做的那样）替换为四阶权重张量$\mathsf{W}$。假设$\mathbf{U}$包含偏置参数，我们可以将全连接层形式化地表示为<br>$$<br>\begin{aligned} \left[\mathbf{H}\right]<em>{i, j} &amp;= [\mathbf{U}]</em>{i, j} + \sum_k \sum_l[\mathsf{W}]<em>{i, j, k, l}  [\mathbf{X}]</em>{k, l}\ &amp;=  [\mathbf{U}]<em>{i, j} +<br>\sum_a \sum_b [\mathsf{V}]</em>{i, j, a, b}  [\mathbf{X}]_{i+a, j+b}.\end{aligned}<br>$$</p>
<ul>
<li><p>平移不变性</p>
<p>这意味着检测对象在输入$\mathbf{X}$中的平移，应该仅导致隐藏表示$\mathbf{H}$中的平移。也就是说，$\mathsf{V}$和$\mathbf{U}$实际上不依赖于$(i, j)$的值，即$[\mathsf{V}]<em>{i, j, a, b} = [\mathbf{V}]</em>{a, b}$。并且$\mathbf{U}$是一个常数，比如$u$。因此，我们可以简化$\mathbf{H}$定义为：</p>
<p>$$<br>[\mathbf{H}]<em>{i, j} = u + \sum_a\sum_b [\mathbf{V}]</em>{a, b} [\mathbf{X}]_{i+a, j+b}.<br>$$<br>这就是<em>卷积</em>（convolution）。</p>
</li>
<li><p>如上所述，为了收集用来训练参数$[\mathbf{H}]<em>{i, j}$的相关信息，我们不应偏离到距$(i, j)$很远的地方。这意味着在$|a|&gt; \Delta$或$|b| &gt; \Delta$的范围之外，我们可以设置$[\mathbf{V}]</em>{a, b} = 0$。因此，我们可以将$[\mathbf{H}]<em>{i, j}$重写为<br>$$<br>[\mathbf{H}]</em>{i, j} = u + \sum_{a = -\Delta}^{\Delta} \sum_{b = -\Delta}^{\Delta} [\mathbf{V}]<em>{a, b}  [\mathbf{X}]</em>{i+a, j+b}.<br>$$<br>这就一个<em>卷积层</em>（convolutional layer），而卷积神经网络是包含卷积层的一种特殊的神经网络。</p>
</li>
</ul>
</li>
<li><p>卷积</p>
<blockquote>
<p>具体推导参见：<a target="_blank" rel="noopener" href="http://zh.d2l.ai/chapter_convolutional-neural-networks/why-conv.html">6.1. 从全连接层到卷积 — 动手学深度学习 2.0.0-beta1 documentation (d2l.ai)</a></p>
<p>6.1.3</p>
</blockquote>
<p>在数学中，两个函数（比如$f, g: \mathbb{R}^d \to \mathbb{R}$）之间的“卷积”被定义为</p>
<p>$$<br>(f * g)(\mathbf{x}) = \int f(\mathbf{z}) g(\mathbf{x}-\mathbf{z}) d\mathbf{z}.<br>$$</p>
</li>
</ol>
<h5 id="图像卷积"><a href="#图像卷积" class="headerlink" title="图像卷积"></a>图像卷积</h5><ol>
<li><p>互相关运算</p>
<p>严格来说，卷积层是个错误的叫法，因为它所表达的运算其实是<em>互相关运算</em>（cross-correlation），而不是卷积运算。</p>
<p>我们暂时忽略通道（第三维）这一情况，看看如何处理二维图像数据和隐藏表示。</p>


<p>上图表示的互相关运算： $0\times0+1\times1+3\times2+4\times3=19$</p>
<p>输出大小略小于输入大小，输出大小等于输入大小$n_h \times n_w$减去卷积核大小$k_h \times k_w$，即：<br>$$<br>(n_h-k_h+1) \times (n_w-k_w+1).<br>$$</p>
</li>
<li><p>特征映射和感受野</p>
<p>输出的卷积层有时被称为<em>特征映射</em>（feature map），因为它可以被视为一个输入映射到下一层的空间维度的转换器。在卷积神经网络中，对于某一层的任意元素$x$，其<em>感受野</em>（receptive field）是指在前向传播期间可能影响$x$计算的所有元素（来自所有先前层）。</p>
</li>
</ol>
<h5 id="填充与步幅"><a href="#填充与步幅" class="headerlink" title="填充与步幅"></a>填充与步幅</h5><ol>
<li><p>填充</p>
<p>我们通常使用较小的卷积核，因此对于任何单个卷积，可能只丢失了几个像素。但随着我们应用多个连续卷积层，累计的像素就很多了。解决这个问题的简单方法即为<em>填充</em>（padding）：在输入图像的边界填充元素（通常填充元素是$0$）。</p>
<p>如果我们添加$p_h$行填充（大约一半在顶部，一半在底部）和$p_w$列填充（左侧大约一半，右侧一半），则输出形状将为</p>
<p>$$<br>(n_h-k_h+p_h+1)\times(n_w-k_w+p_w+1)。<br>$$<br>在许多情况下，我们需要设置$p_h=k_h-1$和$p_w=k_w-1$，使输入和输出具有相同的高度和宽度。</p>
<p><strong>对于任何二维张量<code>X</code>，当满足</strong>：</p>
<ol>
<li>卷积核的大小是奇数；</li>
<li>所有边的填充行数和列数相同；</li>
<li>输出与输入具有相同高度和宽度<br>则可以得出：输出<code>Y[i, j]</code>是通过以输入<code>X[i, j]</code>为中心，与卷积核进行互相关计算得到的。</li>
</ol>
</li>
<li><p>步幅</p>
<p>我们将每次滑动元素的数量称为<em>步幅</em>（stride）。</p>
<p>通常，当垂直步幅为$s_h$、水平步幅为$s_w$时，输出形状为</p>
<p>$$<br>\lfloor(n_h-k_h+p_h+s_h)/s_h\rfloor \times \lfloor(n_w-k_w+p_w+s_w)/s_w\rfloor.<br>$$<br>如果我们设置了$p_h=k_h-1$和$p_w=k_w-1$，则输出形状将简化为$\lfloor(n_h+s_h-1)/s_h\rfloor \times \lfloor(n_w+s_w-1)/s_w\rfloor$。</p>
<p>更进一步，如果输入的高度和宽度可以被垂直和水平步幅整除，则输出形状将为$(n_h/s_h) \times (n_w/s_w)$。</p>
</li>
</ol>
<h5 id="多输入多输出通道"><a href="#多输入多输出通道" class="headerlink" title="多输入多输出通道"></a>多输入多输出通道</h5><p>当我们添加通道时，我们的输入和隐藏的表示都变成了三维张量。例如，每个RGB输入图像具有$3\times h\times w$的形状。我们将这个大小为$3$的轴称为<em>通道</em>（channel）维度。本节将更深入地研究具有多输入和多输出通道的卷积核。</p>
<ol>
<li><p>多输入通道</p>
<p>由于输入和卷积核都有$c_i$个通道，我们可以对每个通道输入的二维张量和卷积核的二维张量进行互相关运算，再对通道求和（将$c_i$的结果相加）得到二维张量。这是多通道输入和多输入通道卷积核之间进行二维互相关运算的结果。</p>


<p>阴影部分是第一个输出元素以及用于计算这个输出的输入和核张量元素：$(1\times1+2\times2+4\times3+5\times4)+(0\times0+1\times1+3\times2+4\times3)=56$。</p>
</li>
<li><p>多输出通道</p>
<p>用$c_i$和$c_o$分别表示输入和输出通道的数目，并让$k_h$和$k_w$为卷积核的高度和宽度。为了获得多个通道的输出，我们可以为每个输出通道创建一个形状为$c_i\times k_h\times k_w$的卷积核张量，这样卷积核的形状是$c_o\times c_i\times k_h\times k_w$。在互相关运算中，每个输出通道先获取所有输入通道，再以对应该输出通道的卷积核计算出结果。</p>
</li>
</ol>
<h5 id="汇聚层"><a href="#汇聚层" class="headerlink" title="汇聚层"></a>汇聚层</h5><p><em>汇聚</em>（pooling）层，它具有双重目的：降低卷积层对位置的敏感性，同时降低对空间降采样表示的敏感性。</p>
<ol>
<li><p>最大汇聚层和平均汇聚层</p>
<p>与卷积层类似，汇聚层运算符由一个固定形状的窗口组成，该窗口根据其步幅大小在输入的所有区域上滑动，为固定形状窗口（有时称为<em>汇聚窗口</em>）遍历的每个位置计算一个输出。</p>
<p>我们通常计算汇聚窗口中所有元素的最大值或平均值。这些操作分别称为<em>最大汇聚层</em>（maximum pooling）和<em>平均汇聚层</em>（average pooling）。</p>
</li>
<li><p>多通道</p>
<p><strong>在处理多通道输入数据时，汇聚层在每个输入通道上单独运算，而不是像卷积层一样在通道上对输入进行汇总。 这意味着汇聚层的输出通道数与输入通道数相同</strong>。</p>
</li>
</ol>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://testcross-01.github.io">testcross</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://testcross-01.github.io/2022/08/06/deep_learning/">https://testcross-01.github.io/2022/08/06/deep_learning/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://testcross-01.github.io" target="_blank">Testcross</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/deep-learning/">deep-learning</a><a class="post-meta__tags" href="/tags/machine-learning/">machine-learning</a></div><div class="post_share"><div class="social-share" data-image="/images/cover/53eTB2uiNRlXwFP.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2022/08/11/test/"><img class="prev-cover" src="/images/cover/T7Mu8Aod3egmC4Q.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">test</div></div></a></div><div class="next-post pull-right"><a href="/2020/07/10/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"><img class="next-cover" src="/images/cover/lP3rLNUBaGtSVzc.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">多线程</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://avatars.githubusercontent.com/u/59674912?v=4" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">testcross</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">7</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">4</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">3</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/testcross-01"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/testcross-01" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:testcross0802@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">testcross的blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.</span> <span class="toc-text">深度学习</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80"><span class="toc-number">1.1.</span> <span class="toc-text">机器学习学习基础</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E6%9C%AF%E8%AF%AD"><span class="toc-number">1.1.1.</span> <span class="toc-text">基本术语</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.1.2.</span> <span class="toc-text">监督学习</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.1.3.</span> <span class="toc-text">无监督学习</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0"><span class="toc-number">1.1.4.</span> <span class="toc-text">模型评估</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="toc-number">1.2.</span> <span class="toc-text">线性回归</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5"><span class="toc-number">1.2.1.</span> <span class="toc-text">线性回归的基本概念</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9A%84%E5%AE%9E%E7%8E%B0"><span class="toc-number">1.2.2.</span> <span class="toc-text">线性回归的实现</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#softmax%E5%9B%9E%E5%BD%92%EF%BC%88%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%EF%BC%89"><span class="toc-number">1.2.3.</span> <span class="toc-text">softmax回归（分类问题）</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA"><span class="toc-number">1.3.</span> <span class="toc-text">多层感知机</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5"><span class="toc-number">1.3.1.</span> <span class="toc-text">多层感知机基本概念</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9%E3%80%81%E6%AC%A0%E6%8B%9F%E5%90%88%E5%92%8C%E8%BF%87%E6%8B%9F%E5%90%88"><span class="toc-number">1.3.2.</span> <span class="toc-text">模型选择、欠拟合和过拟合</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8F"><span class="toc-number">1.3.3.</span> <span class="toc-text">权重衰减</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%9A%82%E9%80%80%E6%B3%95"><span class="toc-number">1.3.4.</span> <span class="toc-text">暂退法</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD%E3%80%81%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-number">1.3.5.</span> <span class="toc-text">前向传播、反向传播</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%95%B0%E5%80%BC%E7%A8%B3%E5%AE%9A%E6%80%A7%E5%92%8C%E6%A8%A1%E5%9E%8B%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-number">1.3.6.</span> <span class="toc-text">数值稳定性和模型初始化</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%8E%AF%E5%A2%83%E5%92%8C%E5%88%86%E5%B8%83%E5%81%8F%E7%A7%BB%EF%BC%88%E6%9A%82%E6%97%B6%E6%9C%AA%E5%BC%84%E6%87%82%EF%BC%89"><span class="toc-number">1.3.7.</span> <span class="toc-text">环境和分布偏移（暂时未弄懂）</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">1.4.</span> <span class="toc-text">卷积神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%BB%8E%E5%85%A8%E9%93%BE%E6%8E%A5%E5%B1%82%E5%88%B0%E5%8D%B7%E7%A7%AF"><span class="toc-number">1.4.1.</span> <span class="toc-text">从全链接层到卷积</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%9B%BE%E5%83%8F%E5%8D%B7%E7%A7%AF"><span class="toc-number">1.4.2.</span> <span class="toc-text">图像卷积</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%A1%AB%E5%85%85%E4%B8%8E%E6%AD%A5%E5%B9%85"><span class="toc-number">1.4.3.</span> <span class="toc-text">填充与步幅</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%A4%9A%E8%BE%93%E5%85%A5%E5%A4%9A%E8%BE%93%E5%87%BA%E9%80%9A%E9%81%93"><span class="toc-number">1.4.4.</span> <span class="toc-text">多输入多输出通道</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%B1%87%E8%81%9A%E5%B1%82"><span class="toc-number">1.4.5.</span> <span class="toc-text">汇聚层</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2022/08/13/String/" title="String"><img src="/images/cover/rJbFpE65tmxPv7R.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="String"/></a><div class="content"><a class="title" href="/2022/08/13/String/" title="String">String</a><time datetime="2022-08-13T09:45:10.234Z" title="发表于 2022-08-13 17:45:10">2022-08-13</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/08/11/test/" title="test"><img src="/images/cover/T7Mu8Aod3egmC4Q.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="test"/></a><div class="content"><a class="title" href="/2022/08/11/test/" title="test">test</a><time datetime="2022-08-11T03:17:59.000Z" title="发表于 2022-08-11 11:17:59">2022-08-11</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/08/06/deep_learning/" title="deep-learning"><img src="/images/cover/53eTB2uiNRlXwFP.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="deep-learning"/></a><div class="content"><a class="title" href="/2022/08/06/deep_learning/" title="deep-learning">deep-learning</a><time datetime="2022-08-06T07:03:06.000Z" title="发表于 2022-08-06 15:03:06">2022-08-06</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2020/07/10/%E5%A4%9A%E7%BA%BF%E7%A8%8B/" title="多线程"><img src="/images/cover/lP3rLNUBaGtSVzc.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="多线程"/></a><div class="content"><a class="title" href="/2020/07/10/%E5%A4%9A%E7%BA%BF%E7%A8%8B/" title="多线程">多线程</a><time datetime="2020-07-10T05:19:06.000Z" title="发表于 2020-07-10 13:19:06">2020-07-10</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2020/06/27/%E5%86%85%E9%83%A8%E7%B1%BB/" title="内部类"><img src="/images/cover/T7Mu8Aod3egmC4Q.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="内部类"/></a><div class="content"><a class="title" href="/2020/06/27/%E5%86%85%E9%83%A8%E7%B1%BB/" title="内部类">内部类</a><time datetime="2020-06-27T06:06:42.000Z" title="发表于 2020-06-27 14:06:42">2020-06-27</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2022 By testcross</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>